{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, diff, sqrt\n",
    "\n",
    "# Define symbols\n",
    "x = symbols('x')\n",
    "\n",
    "# Define the function and its first derivative\n",
    "f_prime = 7*x / sqrt(7*x**2 + 4)\n",
    "\n",
    "# Compute the second derivative using the quotient rule via differentiation\n",
    "f_double_prime = diff(f_prime, x)\n",
    "\n",
    "f_double_prime.simplify()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852656f8",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b31758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa50dc",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and print the data set\n",
    "real_estate_data = pd.read_csv(\"../../data/Real_estate.csv\")\n",
    "print (real_estate_data.shape)\n",
    "real_estate_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a44de6d",
   "metadata": {},
   "source": [
    "### Spltting data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12562c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = real_estate_data.iloc[:, 1:7]\n",
    "y = real_estate_data.iloc[:, -1]\n",
    "\n",
    "# Normalize the features to the [0, 1] range using min max\n",
    "x_max = X.max(axis=0)\n",
    "x_min = X.min(axis=0)\n",
    "X_normalized = (X - x_min) / (x_max - x_min) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b5b5d",
   "metadata": {},
   "source": [
    "### 2. a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85dfd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "w = np.array([1, 1, 1, 1, 1, 1])\n",
    "b = 10\n",
    "learning_rate = .001\n",
    "steps = 4140\n",
    "loss_values = []\n",
    "for step in range(steps):\n",
    "        \n",
    "        y_pred = X_normalized.dot(w) + b\n",
    "        loss = np.sum((y - y_pred) ** 2) / len(y)\n",
    "        loss_values.append(loss) \n",
    "        \n",
    "        gradient_w = -2 * (X_normalized.T.dot(y - y_pred) / len(y))\n",
    "        gradient_b = -2 * (np.sum(y - y_pred) / len(y))\n",
    "        \n",
    "        w = w - (learning_rate * gradient_w)\n",
    "        b = b - (learning_rate * gradient_b)\n",
    "        \n",
    "# Calculate R-squared using final predictions\n",
    "SSR = ((y - y_pred) ** 2).sum()\n",
    "SST = ((y - y.mean()) ** 2).sum()\n",
    "r2 = 1 - (SSR / SST)\n",
    "\n",
    "print(\"Weights: \\n\", w, \"\\n\")\n",
    "print(\"Bias: \", b, \"\\n\")\n",
    "print(\"R^2: \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dace66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss over iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_values, label='Loss (RSS)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss During Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a364d",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21c18f",
   "metadata": {},
   "source": [
    "#### With a sufficiently large number of steps, gradient descent is expected to converge to a solution that is not the same but it is close to the optimal, as evidenced by the loss graph which shows a rapid initial decrease in cost and subsequent stabilization, indicating that the algorithm is approaching an optimal set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a98637",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e8b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "w = np.ones(6)\n",
    "b = 10\n",
    "learning_rate = 0.001\n",
    "steps = 4140\n",
    "n_points = len(y)\n",
    "loss_values = []\n",
    "\n",
    "# Perform Stochastic Gradient Descent\n",
    "for step in range(steps):\n",
    "    i = step % n_points\n",
    "    x_i = X_normalized.iloc[i]\n",
    "    y_i = y.iloc[i]\n",
    "    y_pred_i = np.dot(w, x_i) + b\n",
    "    \n",
    "    loss = (y_i - y_pred_i) ** 2\n",
    "    loss_values.append(loss) \n",
    "    \n",
    "    gradient_w_i = -2 * x_i * (y_i - y_pred_i)\n",
    "    gradient_b_i = -2 * (y_i - y_pred_i)\n",
    "    \n",
    "    w = w - learning_rate * gradient_w_i\n",
    "    b = b - learning_rate * gradient_b_i\n",
    "\n",
    "# Calculate predictions for the entire dataset with final w and b\n",
    "final_y_pred = X_normalized.dot(w) + b\n",
    "\n",
    "# Calculate R-squared using final predictions\n",
    "SSR = ((y - final_y_pred) ** 2).sum()\n",
    "SST = ((y - y.mean()) ** 2).sum()\n",
    "r2 = 1 - (SSR / SST)\n",
    "\n",
    "print(\"Weights: \\n\", w, \"\\n\")\n",
    "print(\"Bias: \", b, \"\\n\")\n",
    "print(\"R^2: \", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3de25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
